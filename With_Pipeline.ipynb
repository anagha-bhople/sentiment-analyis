{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import spacy\n",
    "import unicodedata\n",
    "import en_core_web_sm\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score, auc, roc_curve, precision_recall_curve\n",
    "from textblob import TextBlob\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, log_loss\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('C:/Users/anagha.bhople/Desktop/Sentiment Analysis/preprocess_womens_clothing_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>Review Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended IND</th>\n",
       "      <th>Positive Feedback Count</th>\n",
       "      <th>Review Text Precrocessed</th>\n",
       "      <th>Label</th>\n",
       "      <th>Label_3C</th>\n",
       "      <th>Review Text no Stopwords</th>\n",
       "      <th>tokenized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Absolutely wonderful - silky and sexy and comf...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>absolutely wonderful silk and sex and comfortable</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>absolutely wonderful silk sex comfortable</td>\n",
       "      <td>['absolutely', 'wonderful', 'silk', 'sex', 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Love this dress!  it's sooo pretty.  i happene...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>love this dress -PRON- is soon pretty i happen...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>love dress soon pretty happen find store am gl...</td>\n",
       "      <td>['love', 'dress', 'soon', 'pretty', 'happen', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index                                        Review Text  \\\n",
       "0           0      0  Absolutely wonderful - silky and sexy and comf...   \n",
       "1           1      1  Love this dress!  it's sooo pretty.  i happene...   \n",
       "\n",
       "   Rating  Recommended IND  Positive Feedback Count  \\\n",
       "0       4                1                        0   \n",
       "1       5                1                        4   \n",
       "\n",
       "                            Review Text Precrocessed  Label  Label_3C  \\\n",
       "0  absolutely wonderful silk and sex and comfortable      1         1   \n",
       "1  love this dress -PRON- is soon pretty i happen...      1         1   \n",
       "\n",
       "                            Review Text no Stopwords  \\\n",
       "0          absolutely wonderful silk sex comfortable   \n",
       "1  love dress soon pretty happen find store am gl...   \n",
       "\n",
       "                                      tokenized_text  \n",
       "0  ['absolutely', 'wonderful', 'silk', 'sex', 'co...  \n",
       "1  ['love', 'dress', 'soon', 'pretty', 'happen', ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df['Review Text Precrocessed'], df['Label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x= TextBlob(str(x)).words\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['a', 'about', 'an', 'and', 'are', 'as', 'at', 'be', 'been', 'but', 'by', 'can', \\\n",
    "             'even', 'ever', 'for', 'from', 'get', 'had', 'has', 'have', 'he', 'her', 'hers', 'his', \\\n",
    "             'how', 'i', 'if', 'in', 'into', 'is', 'it', 'its', 'just', 'me', 'my', 'of', 'on', 'or', \\\n",
    "             'see', 'seen', 'she', 'so', 'than', 'that', 'the', 'their', 'there', 'they', 'this', \\\n",
    "             'to', 'was', 'we', 'were', 'what', 'when', 'which', 'who', 'will', 'with', 'you','-PRON-']\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(stop_words=stopwords, tokenizer= tokenize, ngram_range=(1, 2), max_df=0.9, min_df=3, sublinear_tf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_pipeline = Pipeline([\n",
    "        ('vectorizer', tfidf),\n",
    "        ('classifier', classifier)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:383: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['pron'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 TfidfVectorizer(max_df=0.9, min_df=3, ngram_range=(1, 2),\n",
       "                                 stop_words=['a', 'about', 'an', 'and', 'are',\n",
       "                                             'as', 'at', 'be', 'been', 'but',\n",
       "                                             'by', 'can', 'even', 'ever', 'for',\n",
       "                                             'from', 'get', 'had', 'has',\n",
       "                                             'have', 'he', 'her', 'hers', 'his',\n",
       "                                             'how', 'i', 'if', 'in', 'into',\n",
       "                                             'is', ...],\n",
       "                                 sublinear_tf=True,\n",
       "                                 tokenizer=<function tokenize at 0x000002512A556040>)),\n",
       "                ('classifier', LogisticRegression())])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = sentiment_pipeline.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n"
     ]
    }
   ],
   "source": [
    "Classification_report = classification_report(y_test, y_pred)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "Accuracy_score = accuracy_score(y_test, y_pred)\n",
    "print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "F1_score = f1_score(y_test, y_pred)\n",
    "print('F1 Score: {}'.format(F1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "model score: 0.822\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n",
      "SVC(C=0.025, probability=True)\n",
      "model score: 0.775\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n",
      "DecisionTreeClassifier()\n",
      "model score: 0.786\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n",
      "RandomForestClassifier()\n",
      "model score: 0.842\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n",
      "AdaBoostClassifier()\n",
      "model score: 0.844\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n",
      "GradientBoostingClassifier()\n",
      "model score: 0.844\n",
      "[[ 729  542]\n",
      " [ 151 4239]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.57      0.68      1271\n",
      "           1       0.89      0.97      0.92      4390\n",
      "\n",
      "    accuracy                           0.88      5661\n",
      "   macro avg       0.86      0.77      0.80      5661\n",
      "weighted avg       0.87      0.88      0.87      5661\n",
      "\n",
      "Accuracy Score: 0.8775834658187599\n",
      "F1 Score: 0.9244357212953876\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', tfidf),\n",
    "                      ('classifier', classifier)])\n",
    "    pipe.fit(X_train, y_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % pipe.score(X_test, y_test))\n",
    "    Classification_report = classification_report(y_test, y_pred)\n",
    "    \n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    Accuracy_score = accuracy_score(y_test, y_pred)\n",
    "    print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "    F1_score = f1_score(y_test, y_pred)\n",
    "    print('F1 Score: {}'.format(F1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(x):\n",
    "    x= TextBlob(str(x)).words\n",
    "    return x\n",
    "df['Review Text w2v']=df['Review Text Precrocessed'].apply(lambda x: tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df['Review Text w2v']\n",
    "y=df['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_Y=y\n",
    "w2v_X=X\n",
    "w2vX_train, w2vX_test, w2vy_train, w2vy_test = train_test_split(w2v_X,w2v_Y, test_size=.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22642,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(X, size=550, window=50, min_count=30, workers=10, sg=0, iter=200)\n",
    "#w2v = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MeanEmbeddingVectorizer(object):\n",
    "#     def __init__(self, word2vec):\n",
    "#         self.word2vec = word2vec\n",
    "#         # if a text is empty we should return a vector of zeros\n",
    "#         # with the same dimensionality as all the other vectors\n",
    "#         self.dim = len(word2vec.itervalues().next())\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         return self\n",
    "    \n",
    "#     def document_vector(doc):\n",
    "#         doc = [word for word in doc if word in model.wv.vocab]\n",
    "#         return np.mean(model[doc], axis=0)\n",
    "\n",
    "#     def transform(self, X):\n",
    "#         return X.apply(document_vector)\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        \n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \n",
    "        asd= np.array([\n",
    "            np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
    "            for words in X\n",
    "        ])\n",
    "        \n",
    "        print(X.shape)\n",
    "        print(asd.shape)\n",
    "        return asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "etree_w2v = Pipeline([\n",
    "    (\"word2vec vectorizer\", MeanEmbeddingVectorizer(model)),\n",
    "    ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4529,)\n",
      "(4529, 550)\n",
      "LogisticRegression()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    }
   ],
   "source": [
    "etree_w2v.fit(w2vX_train, w2vy_train)  \n",
    "\n",
    "w2vy_pred = etree_w2v.predict(w2vX_test)\n",
    "\n",
    "print(classifier)\n",
    "print(\"model score: %.3f\" % etree_w2v.score(w2vX_test, w2vy_test))\n",
    "\n",
    "print(confusion_matrix(w2vy_test, w2vy_pred))\n",
    "print(classification_report(w2vy_test, w2vy_pred))\n",
    "Accuracy_score = accuracy_score(w2vy_test, w2vy_pred)\n",
    "print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "F1_score = f1_score(w2vy_test, w2vy_pred)\n",
    "print('F1 Score: {}'.format(F1_score))\n",
    "Average_precision = average_precision_score(w2vy_test, w2vy_pred)\n",
    "print('Average precision-recall score: {}'.format(Average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNeighborsClassifier(n_neighbors=3)\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC(C=0.025, probability=True)\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier()\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier()\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier()\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18113,)\n",
      "(18113, 550)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n",
      "<ipython-input-20-ceca36e667ed>:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  np.mean([self.word2vec[w] for w in words if w in self.word2vec], axis=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GradientBoostingClassifier()\n",
      "(4529,)\n",
      "(4529, 550)\n",
      "model score: 0.877\n",
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n",
      "Average precision-recall score: 0.8960436276175099\n"
     ]
    }
   ],
   "source": [
    "classifiers = [\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"rbf\", C=0.025, probability=True),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    GradientBoostingClassifier()\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[('preprocessor', MeanEmbeddingVectorizer(model)),\n",
    "                      ('classifier', classifier)])\n",
    "    etree_w2v.fit(w2vX_train, w2vy_train)   \n",
    "    print(classifier)\n",
    "    print(\"model score: %.3f\" % etree_w2v.score(w2vX_test, w2vy_test))\n",
    "    \n",
    "    print(confusion_matrix(w2vy_test, w2vy_pred))\n",
    "    print(classification_report(w2vy_test, w2vy_pred))\n",
    "    Accuracy_score = accuracy_score(w2vy_test, w2vy_pred)\n",
    "    print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "    F1_score = f1_score(w2vy_test, w2vy_pred)\n",
    "    print('F1 Score: {}'.format(F1_score))\n",
    "    Average_precision = average_precision_score(w2vy_test, w2vy_pred)\n",
    "    print('Average precision-recall score: {}'.format(Average_precision))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-dca77d387cdc>:4: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  return np.mean(model[doc], axis=0)\n"
     ]
    }
   ],
   "source": [
    "def document_vector(doc):\n",
    "    \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n",
    "    doc = [word for word in doc if word in model.wv.vocab]\n",
    "    return np.mean(model[doc], axis=0)\n",
    "\n",
    "\n",
    "df['doc_vector'] = df['Review Text w2v'].apply(document_vector)\n",
    "#test['doc_vector'] = df['Review Text w2v'].apply(document_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = list(df['doc_vector'])\n",
    "w_Y=df['Label']\n",
    "w_X=X1\n",
    "wX_train, wX_test, wy_train, wy_test = train_test_split(w_X,w_Y, test_size=.20, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogisticRegression_binary_classification(X_train, y_train, X_test, y_test):\n",
    "    logisticRegr = LogisticRegression()\n",
    "    \n",
    "    logisticRegr.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = logisticRegr.predict(X_test)\n",
    "    Confusion_matrix = confusion_matrix(y_test, predictions)\n",
    "    Classification_report = classification_report(y_test, predictions)\n",
    "    print(confusion_matrix(y_test,predictions))\n",
    "    print(classification_report(y_test, predictions))\n",
    "    Accuracy_score = accuracy_score(y_test, predictions)\n",
    "    print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "    F1_score = f1_score(y_test, predictions)\n",
    "    print('F1 Score: {}'.format(F1_score))\n",
    "    return Confusion_matrix, Classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 680  351]\n",
      " [ 204 3294]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71      1031\n",
      "           1       0.90      0.94      0.92      3498\n",
      "\n",
      "    accuracy                           0.88      4529\n",
      "   macro avg       0.84      0.80      0.82      4529\n",
      "weighted avg       0.87      0.88      0.87      4529\n",
      "\n",
      "Accuracy Score: 0.8774563921395452\n",
      "F1 Score: 0.9223015539689206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[ 680,  351],\n",
       "        [ 204, 3294]], dtype=int64),\n",
       " '              precision    recall  f1-score   support\\n\\n           0       0.77      0.66      0.71      1031\\n           1       0.90      0.94      0.92      3498\\n\\n    accuracy                           0.88      4529\\n   macro avg       0.84      0.80      0.82      4529\\nweighted avg       0.87      0.88      0.87      4529\\n')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression_binary_classification(wX_train, wy_train, wX_test, wy_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove with pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2, y2 = df['Review Text no Stopwords'], df['Label']\n",
    "gX_train, gX_test, gy_train, gy_test = train_test_split(X2, y2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "glove_vectors = dict()\n",
    "file = open('C:/Users/anagha.bhople/Desktop/Sentiment Analysis/GloVe Word Embedding/glove/glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "for line in file:\n",
    "    values = line.split()\n",
    "    \n",
    "    word  = values[0]\n",
    "    vectors = np.asarray(values[1:])\n",
    "    glove_vectors[word] = vectors\n",
    "    \n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "global arr \n",
    "arr= np.zeros(100)\n",
    "\n",
    "class glove(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        global arr \n",
    "        arr= np.zeros(100)\n",
    "\n",
    "    def fit(self, X2, y2):\n",
    "        return self\n",
    "    \n",
    "    def get_vec(self, X2):\n",
    "        arr = np.zeros(100)\n",
    "        text = str(X2).split()\n",
    "\n",
    "        for t in text:\n",
    "            #print(t)\n",
    "            try:\n",
    "                vec = self.word2vec.get(t).astype(float)\n",
    "                arr = arr + vec\n",
    "            except:\n",
    "                pass\n",
    "        arr = arr.reshape(1, -1)[0]\n",
    "        #print(arr.shape)\n",
    "        return arr/len(text)\n",
    "\n",
    "\n",
    "    def transform(self, X2):\n",
    "        lis=np.array(X2.apply(lambda x: self.get_vec(x)))\n",
    "        #print(lis.shape)\n",
    "        lis = lis.reshape(1, -1)[0]\n",
    "        lis = np.concatenate(lis, axis = 0).reshape(-1, 100)\n",
    "        #print(lis.shape)\n",
    "        return lis\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "glo = Pipeline([\n",
    "    (\"word2vec vectorizer\", glove(glove_vectors)),\n",
    "    ('classifier', classifier)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16981,)\n",
      "(16981, 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5661,)\n",
      "(5661, 100)\n",
      "LogisticRegression()\n",
      "(5661,)\n",
      "(5661, 100)\n",
      "model score: 0.818\n",
      "[[ 487  784]\n",
      " [ 248 4142]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.38      0.49      1271\n",
      "           1       0.84      0.94      0.89      4390\n",
      "\n",
      "    accuracy                           0.82      5661\n",
      "   macro avg       0.75      0.66      0.69      5661\n",
      "weighted avg       0.80      0.82      0.80      5661\n",
      "\n",
      "Accuracy Score: 0.8177000529941706\n",
      "F1 Score: 0.8892228424216402\n",
      "Average precision-recall score: 0.8371520025776354\n"
     ]
    }
   ],
   "source": [
    "glo.fit(gX_train, gy_train)  \n",
    "\n",
    "gy_pred = glo.predict(gX_test)\n",
    "\n",
    "print(classifier)\n",
    "print(\"model score: %.3f\" % glo.score(gX_test, gy_test))\n",
    "\n",
    "print(confusion_matrix(gy_test, gy_pred))\n",
    "print(classification_report(gy_test, gy_pred))\n",
    "Accuracy_score = accuracy_score(gy_test, gy_pred)\n",
    "print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "F1_score = f1_score(gy_test, gy_pred)\n",
    "print('F1 Score: {}'.format(F1_score))\n",
    "Average_precision = average_precision_score(gy_test, gy_pred)\n",
    "print('Average precision-recall score: {}'.format(Average_precision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier()\n",
    "    ]\n",
    "\n",
    "for classifier in classifiers:\n",
    "    pipe = Pipeline(steps=[(\"word2vec vectorizer\", glove(glove_vectors)),\n",
    "                      ('classifier', classifier)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:278: UserWarning: The total space of parameters 2 is smaller than n_iter=10. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Create dictionary with candidate learning algorithms and their hyperparameters\n",
    "search_space = [\n",
    " {\"classifier\": [RandomForestClassifier()],\n",
    " \"classifier__max_features\": [1, 2]}]\n",
    "\n",
    "# Create grid search\n",
    "randomizedsearch = RandomizedSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "# Fit grid search\n",
    "best_model = randomizedsearch.fit(gX_train, gy_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('word2vec vectorizer', <__main__.glove at 0x25417455610>),\n",
       "  ('classifier', RandomForestClassifier(max_features=2))],\n",
       " 'verbose': False,\n",
       " 'word2vec vectorizer': <__main__.glove at 0x25417455610>,\n",
       " 'classifier': RandomForestClassifier(max_features=2),\n",
       " 'classifier__bootstrap': True,\n",
       " 'classifier__ccp_alpha': 0.0,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__criterion': 'gini',\n",
       " 'classifier__max_depth': None,\n",
       " 'classifier__max_features': 2,\n",
       " 'classifier__max_leaf_nodes': None,\n",
       " 'classifier__max_samples': None,\n",
       " 'classifier__min_impurity_decrease': 0.0,\n",
       " 'classifier__min_impurity_split': None,\n",
       " 'classifier__min_samples_leaf': 1,\n",
       " 'classifier__min_samples_split': 2,\n",
       " 'classifier__min_weight_fraction_leaf': 0.0,\n",
       " 'classifier__n_estimators': 100,\n",
       " 'classifier__n_jobs': None,\n",
       " 'classifier__oob_score': False,\n",
       " 'classifier__random_state': None,\n",
       " 'classifier__verbose': 0,\n",
       " 'classifier__warm_start': False}"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.best_estimator_.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score: 0.787\n",
      "[[  96 1175]\n",
      " [  31 4359]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.08      0.14      1271\n",
      "           1       0.79      0.99      0.88      4390\n",
      "\n",
      "    accuracy                           0.79      5661\n",
      "   macro avg       0.77      0.53      0.51      5661\n",
      "weighted avg       0.78      0.79      0.71      5661\n",
      "\n",
      "Accuracy Score: 0.7869634340222575\n",
      "F1 Score: 0.8784764207980653\n",
      "Average precision-recall score: 0.7875900698301181\n"
     ]
    }
   ],
   "source": [
    "gy_pred = best_model.predict(gX_test)\n",
    "\n",
    "print(\"model score: %.3f\" % best_model.score(gX_test, gy_test))\n",
    "\n",
    "print(confusion_matrix(gy_test, gy_pred))\n",
    "print(classification_report(gy_test, gy_pred))\n",
    "Accuracy_score = accuracy_score(gy_test, gy_pred)\n",
    "print('Accuracy Score: {}'.format(Accuracy_score))\n",
    "F1_score = f1_score(gy_test, gy_pred)\n",
    "print('F1 Score: {}'.format(F1_score))\n",
    "Average_precision = average_precision_score(gy_test, gy_pred)\n",
    "print('Average precision-recall score: {}'.format(Average_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_LR = Pipeline(steps=[(\"word2vec vectorizer\", glove(glove_vectors)),\n",
    "                      ('clf', LogisticRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_RF = Pipeline(steps=[(\"word2vec vectorizer\", glove(glove_vectors)),\n",
    "                      ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_SVM = Pipeline(steps=[(\"word2vec vectorizer\", glove(glove_vectors)),\n",
    "                      ('clf', svm.SVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Logistic Regression\n"
     ]
    }
   ],
   "source": [
    "# Set grid search params\n",
    "param_range = [9, 10]\n",
    "param_range_fl = [1.0, 0.5]\n",
    "\n",
    "grid_params_lr = [{'clf__penalty': ['l1', 'l2'],\n",
    "        'clf__C': param_range_fl,\n",
    "        'clf__solver': ['liblinear']}] \n",
    "\n",
    "\n",
    "# grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "#         'clf__max_depth': param_range,\n",
    "#         'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "grid_params_svm = [{'clf__kernel': ['linear', 'rbf'], \n",
    "        'clf__C': param_range}]\n",
    "\n",
    "# Construct grid searches\n",
    "jobs = -1\n",
    "\n",
    "LR = GridSearchCV(estimator=pipe_LR,\n",
    "            param_grid=grid_params_lr,\n",
    "            scoring='accuracy',\n",
    "            cv=10) \n",
    "\n",
    "\n",
    "\n",
    "# RF = GridSearchCV(estimator=pipe_rf,\n",
    "#             param_grid=grid_params_rf,\n",
    "#             scoring='accuracy',\n",
    "#             cv=10, \n",
    "#             n_jobs=jobs)\n",
    "\n",
    "\n",
    "SVM = GridSearchCV(estimator=pipe_SVM,\n",
    "            param_grid=grid_params_svm,\n",
    "            scoring='accuracy',\n",
    "            cv=10,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "\n",
    "\n",
    "# List of pipelines for iterating through each of them\n",
    "grids = [LR,SVM]\n",
    "\n",
    "# Creating a dict for our reference\n",
    "grid_dict = {0: 'Logistic Regression', \n",
    "        1: 'Support Vector Machine'}\n",
    "\n",
    "# Fit the grid search objects\n",
    "print('Performing model optimizations...')\n",
    "best_acc = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    gs.fit(gX_train, gy_train)\n",
    "    print('Best params are : %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    gy_pred = gs.predict(gX_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % accuracy_score(gy_test, gy_pred))\n",
    "    # Track best (highest test accuracy) model\n",
    "    if accuracy_score(gy_test, gy_pred) > best_acc:\n",
    "        best_acc = accuracy_score(gy_test, gy_pred)\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])\n",
    "\n",
    "\n",
    "\n",
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_grid_search_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
